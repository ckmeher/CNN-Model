{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_MNIST.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMge/crT3THrUUybdxQneBv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckmeher/CNN-Model/blob/main/VAE_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import tensorflow \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "Ls0MxHgNMweH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BPwKBGXpIh9L"
      },
      "outputs": [],
      "source": [
        "img_size = 28\n",
        "num_channels = 1\n",
        "latent_space_dim = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building the Encoder"
      ],
      "metadata": {
        "id": "n5OFwQER2_Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "x = tensorflow.keras.layers.Input(shape=(img_size, img_size, num_channels), name=\"encoder_input\")\n",
        "\n",
        "encoder_conv_layer1 = tensorflow.keras.layers.Conv2D(filters=1, kernel_size=(3, 3), padding=\"same\", strides=1, name=\"encoder_conv_1\")(x)\n",
        "encoder_norm_layer1 = tensorflow.keras.layers.BatchNormalization(name=\"encoder_norm_1\")(encoder_conv_layer1)\n",
        "encoder_activ_layer1 = tensorflow.keras.layers.LeakyReLU(name=\"encoder_leakyrelu_1\")(encoder_norm_layer1)\n",
        "\n",
        "encoder_conv_layer2 = tensorflow.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", strides=1, name=\"encoder_conv_2\")(encoder_activ_layer1)\n",
        "encoder_norm_layer2 = tensorflow.keras.layers.BatchNormalization(name=\"encoder_norm_2\")(encoder_conv_layer2)\n",
        "encoder_activ_layer2 = tensorflow.keras.layers.LeakyReLU(name=\"encoder_activ_layer_2\")(encoder_norm_layer2)\n",
        "\n",
        "encoder_conv_layer3 = tensorflow.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", strides=2, name=\"encoder_conv_3\")(encoder_activ_layer2)\n",
        "encoder_norm_layer3 = tensorflow.keras.layers.BatchNormalization(name=\"encoder_norm_3\")(encoder_conv_layer3)\n",
        "encoder_activ_layer3 = tensorflow.keras.layers.LeakyReLU(name=\"encoder_activ_layer_3\")(encoder_norm_layer3)\n",
        "\n",
        "encoder_conv_layer4 = tensorflow.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", strides=2, name=\"encoder_conv_4\")(encoder_activ_layer3)\n",
        "encoder_norm_layer4 = tensorflow.keras.layers.BatchNormalization(name=\"encoder_norm_4\")(encoder_conv_layer4)\n",
        "encoder_activ_layer4 = tensorflow.keras.layers.LeakyReLU(name=\"encoder_activ_layer_4\")(encoder_norm_layer4)\n",
        "\n",
        "encoder_conv_layer5 = tensorflow.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", strides=1, name=\"encoder_conv_5\")(encoder_activ_layer4)\n",
        "encoder_norm_layer5 = tensorflow.keras.layers.BatchNormalization(name=\"encoder_norm_5\")(encoder_conv_layer5)\n",
        "encoder_activ_layer5 = tensorflow.keras.layers.LeakyReLU(name=\"encoder_activ_layer_5\")(encoder_norm_layer5)\n",
        "\n",
        "shape_before_flatten = tensorflow.keras.backend.int_shape(encoder_activ_layer5)[1:]\n",
        "encoder_flatten = tensorflow.keras.layers.Flatten()(encoder_activ_layer5)\n",
        "\n",
        "encoder_mu = tensorflow.keras.layers.Dense(units=latent_space_dim, name=\"encoder_mu\")(encoder_flatten)\n",
        "encoder_log_variance = tensorflow.keras.layers.Dense(units=latent_space_dim, name=\"encoder_log_variance\")(encoder_flatten)\n",
        "\n",
        "encoder_mu_log_variance_model = tensorflow.keras.models.Model(x, (encoder_mu, encoder_log_variance), name=\"encoder_mu_log_variance_model\")\n",
        "\n",
        "def sampling(mu_log_variance):\n",
        "    mu, log_variance = mu_log_variance\n",
        "    epsilon = tensorflow.keras.backend.random_normal(shape=tensorflow.keras.backend.shape(mu), mean=0.0, stddev=1.0)\n",
        "    random_sample = mu + tensorflow.keras.backend.exp(log_variance/2) * epsilon\n",
        "    return random_sample\n",
        "\n",
        "encoder_output = tensorflow.keras.layers.Lambda(sampling, name=\"encoder_output\")([encoder_mu, encoder_log_variance])\n",
        "\n",
        "encoder = tensorflow.keras.models.Model(x, encoder_output, name=\"encoder_model\")\n",
        "encoder.summary()"
      ],
      "metadata": {
        "id": "I7w-b1ISItsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "473ff7f5-0c5b-4174-875d-5b061f197cca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_input (InputLayer)     [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " encoder_conv_1 (Conv2D)        (None, 28, 28, 1)    10          ['encoder_input[0][0]']          \n",
            "                                                                                                  \n",
            " encoder_norm_1 (BatchNormaliza  (None, 28, 28, 1)   4           ['encoder_conv_1[0][0]']         \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " encoder_leakyrelu_1 (LeakyReLU  (None, 28, 28, 1)   0           ['encoder_norm_1[0][0]']         \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " encoder_conv_2 (Conv2D)        (None, 28, 28, 32)   320         ['encoder_leakyrelu_1[0][0]']    \n",
            "                                                                                                  \n",
            " encoder_norm_2 (BatchNormaliza  (None, 28, 28, 32)  128         ['encoder_conv_2[0][0]']         \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " encoder_activ_layer_2 (LeakyRe  (None, 28, 28, 32)  0           ['encoder_norm_2[0][0]']         \n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " encoder_conv_3 (Conv2D)        (None, 14, 14, 64)   18496       ['encoder_activ_layer_2[0][0]']  \n",
            "                                                                                                  \n",
            " encoder_norm_3 (BatchNormaliza  (None, 14, 14, 64)  256         ['encoder_conv_3[0][0]']         \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " encoder_activ_layer_3 (LeakyRe  (None, 14, 14, 64)  0           ['encoder_norm_3[0][0]']         \n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " encoder_conv_4 (Conv2D)        (None, 7, 7, 64)     36928       ['encoder_activ_layer_3[0][0]']  \n",
            "                                                                                                  \n",
            " encoder_norm_4 (BatchNormaliza  (None, 7, 7, 64)    256         ['encoder_conv_4[0][0]']         \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " encoder_activ_layer_4 (LeakyRe  (None, 7, 7, 64)    0           ['encoder_norm_4[0][0]']         \n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " encoder_conv_5 (Conv2D)        (None, 7, 7, 64)     36928       ['encoder_activ_layer_4[0][0]']  \n",
            "                                                                                                  \n",
            " encoder_norm_5 (BatchNormaliza  (None, 7, 7, 64)    256         ['encoder_conv_5[0][0]']         \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " encoder_activ_layer_5 (LeakyRe  (None, 7, 7, 64)    0           ['encoder_norm_5[0][0]']         \n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 3136)         0           ['encoder_activ_layer_5[0][0]']  \n",
            "                                                                                                  \n",
            " encoder_mu (Dense)             (None, 2)            6274        ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " encoder_log_variance (Dense)   (None, 2)            6274        ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " encoder_output (Lambda)        (None, 2)            0           ['encoder_mu[0][0]',             \n",
            "                                                                  'encoder_log_variance[0][0]']   \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 106,130\n",
            "Trainable params: 105,680\n",
            "Non-trainable params: 450\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building the Decoder"
      ],
      "metadata": {
        "id": "tgNodB963QCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoder\n",
        "\n",
        "decoder_input = tensorflow.keras.layers.Input(shape=(latent_space_dim), name=\"decoder_input\")\n",
        "decoder_dense_layer1 = tensorflow.keras.layers.Dense(units=numpy.prod(shape_before_flatten), name=\"decoder_dense_1\")(decoder_input)\n",
        "decoder_reshape = tensorflow.keras.layers.Reshape(target_shape=shape_before_flatten)(decoder_dense_layer1)\n",
        "\n",
        "decoder_conv_tran_layer1 = tensorflow.keras.layers.Conv2DTranspose(filters=64, kernel_size=(3, 3), padding=\"same\", strides=1, name=\"decoder_conv_tran_1\")(decoder_reshape)\n",
        "decoder_norm_layer1 = tensorflow.keras.layers.BatchNormalization(name=\"decoder_norm_1\")(decoder_conv_tran_layer1)\n",
        "decoder_activ_layer1 = tensorflow.keras.layers.LeakyReLU(name=\"decoder_leakyrelu_1\")(decoder_norm_layer1)\n",
        "\n",
        "decoder_conv_tran_layer2 = tensorflow.keras.layers.Conv2DTranspose(filters=64, kernel_size=(3, 3), padding=\"same\", strides=2, name=\"decoder_conv_tran_2\")(decoder_activ_layer1)\n",
        "decoder_norm_layer2 = tensorflow.keras.layers.BatchNormalization(name=\"decoder_norm_2\")(decoder_conv_tran_layer2)\n",
        "decoder_activ_layer2 = tensorflow.keras.layers.LeakyReLU(name=\"decoder_leakyrelu_2\")(decoder_norm_layer2)\n",
        "\n",
        "decoder_conv_tran_layer3 = tensorflow.keras.layers.Conv2DTranspose(filters=64, kernel_size=(3, 3), padding=\"same\", strides=2, name=\"decoder_conv_tran_3\")(decoder_activ_layer2)\n",
        "decoder_norm_layer3 = tensorflow.keras.layers.BatchNormalization(name=\"decoder_norm_3\")(decoder_conv_tran_layer3)\n",
        "decoder_activ_layer3 = tensorflow.keras.layers.LeakyReLU(name=\"decoder_leakyrelu_3\")(decoder_norm_layer3)\n",
        "\n",
        "decoder_conv_tran_layer4 = tensorflow.keras.layers.Conv2DTranspose(filters=1, kernel_size=(3, 3), padding=\"same\", strides=1, name=\"decoder_conv_tran_4\")(decoder_activ_layer3)\n",
        "decoder_output = tensorflow.keras.layers.LeakyReLU(name=\"decoder_output\")(decoder_conv_tran_layer4 )\n",
        "\n",
        "decoder = tensorflow.keras.models.Model(decoder_input, decoder_output, name=\"decoder_model\")\n",
        "decoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s--aC5zE3SnH",
        "outputId": "4a0d95cb-8eb0-4af7-e780-dce01b1e6263"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " decoder_input (InputLayer)  [(None, 2)]               0         \n",
            "                                                                 \n",
            " decoder_dense_1 (Dense)     (None, 3136)              9408      \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 7, 7, 64)          0         \n",
            "                                                                 \n",
            " decoder_conv_tran_1 (Conv2D  (None, 7, 7, 64)         36928     \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " decoder_norm_1 (BatchNormal  (None, 7, 7, 64)         256       \n",
            " ization)                                                        \n",
            "                                                                 \n",
            " decoder_leakyrelu_1 (LeakyR  (None, 7, 7, 64)         0         \n",
            " eLU)                                                            \n",
            "                                                                 \n",
            " decoder_conv_tran_2 (Conv2D  (None, 14, 14, 64)       36928     \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " decoder_norm_2 (BatchNormal  (None, 14, 14, 64)       256       \n",
            " ization)                                                        \n",
            "                                                                 \n",
            " decoder_leakyrelu_2 (LeakyR  (None, 14, 14, 64)       0         \n",
            " eLU)                                                            \n",
            "                                                                 \n",
            " decoder_conv_tran_3 (Conv2D  (None, 28, 28, 64)       36928     \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " decoder_norm_3 (BatchNormal  (None, 28, 28, 64)       256       \n",
            " ization)                                                        \n",
            "                                                                 \n",
            " decoder_leakyrelu_3 (LeakyR  (None, 28, 28, 64)       0         \n",
            " eLU)                                                            \n",
            "                                                                 \n",
            " decoder_conv_tran_4 (Conv2D  (None, 28, 28, 1)        577       \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " decoder_output (LeakyReLU)  (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 121,537\n",
            "Trainable params: 121,153\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VAE using encoder and decoder"
      ],
      "metadata": {
        "id": "Gqoyvj-k3Z71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vae_input = tensorflow.keras.layers.Input(shape=(img_size, img_size, num_channels), name=\"VAE_input\")\n",
        "vae_encoder_output = encoder(vae_input)\n",
        "vae_decoder_output = decoder(vae_encoder_output)\n",
        "vae = tensorflow.keras.models.Model(vae_input, vae_decoder_output, name=\"VAE\")\n",
        "vae.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLmfp03h3eUm",
        "outputId": "2693cfd9-b381-422a-c195-d4b3c9a977ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"VAE\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " VAE_input (InputLayer)      [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " encoder_model (Functional)  (None, 2)                 106130    \n",
            "                                                                 \n",
            " decoder_model (Functional)  (None, 28, 28, 1)         121537    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 227,667\n",
            "Trainable params: 226,833\n",
            "Non-trainable params: 834\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss function and Model Compilation"
      ],
      "metadata": {
        "id": "8G_vmU-i3pi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(encoder_mu, encoder_log_variance):\n",
        "    def vae_reconstruction_loss(y_true, y_predict):\n",
        "        reconstruction_loss_factor = 1000\n",
        "        reconstruction_loss = tensorflow.keras.backend.mean(tensorflow.keras.backend.square(y_true-y_predict), axis=[1, 2, 3])\n",
        "        return reconstruction_loss_factor * reconstruction_loss\n",
        "\n",
        "    def vae_kl_loss(encoder_mu, encoder_log_variance):\n",
        "        kl_loss = -0.5 * tensorflow.keras.backend.sum(1.0 + encoder_log_variance - tensorflow.keras.backend.square(encoder_mu) - tensorflow.keras.backend.exp(encoder_log_variance), axis=1)\n",
        "        return kl_loss\n",
        "\n",
        "    def vae_kl_loss_metric(y_true, y_predict):\n",
        "        kl_loss = -0.5 * tensorflow.keras.backend.sum(1.0 + encoder_log_variance - tensorflow.keras.backend.square(encoder_mu) - tensorflow.keras.backend.exp(encoder_log_variance), axis=1)\n",
        "        return kl_loss\n",
        "\n",
        "    def vae_loss(y_true, y_predict):\n",
        "        reconstruction_loss = vae_reconstruction_loss(y_true, y_predict)\n",
        "        kl_loss = vae_kl_loss(y_true, y_predict)\n",
        "\n",
        "        loss = reconstruction_loss + kl_loss\n",
        "        return loss\n",
        "\n",
        "    return vae_loss\n",
        "\n",
        "vae.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.0005), loss=loss_func(encoder_mu, encoder_log_variance))"
      ],
      "metadata": {
        "id": "bBeOWhxfMq59"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the Data and Training the Model"
      ],
      "metadata": {
        "id": "EMh4e66938bD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "x_train = numpy.reshape(x_train, newshape=(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1))\n",
        "x_test = numpy.reshape(x_test, newshape=(x_test.shape[0], x_train.shape[1], x_train.shape[2], 1))\n",
        "\n",
        "vae.fit(x_train, x_train, epochs=200, batch_size=32, shuffle=True, validation_data=(x_test, x_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPbGoCx0341R",
        "outputId": "afd501fc-d3f3-4ede-d63d-a3572575cc37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/200\n",
            "1875/1875 [==============================] - 43s 17ms/step - loss: 56.6151 - val_loss: 51.5495\n",
            "Epoch 2/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 50.0275 - val_loss: 48.6256\n",
            "Epoch 3/200\n",
            "1875/1875 [==============================] - 31s 17ms/step - loss: 47.9804 - val_loss: 47.0707\n",
            "Epoch 4/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 46.6850 - val_loss: 45.6479\n",
            "Epoch 5/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 45.6919 - val_loss: 45.2722\n",
            "Epoch 6/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 44.9457 - val_loss: 44.5285\n",
            "Epoch 7/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 44.4139 - val_loss: 44.1949\n",
            "Epoch 8/200\n",
            "1875/1875 [==============================] - 31s 17ms/step - loss: 43.8997 - val_loss: 43.3444\n",
            "Epoch 9/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 43.5339 - val_loss: 43.9392\n",
            "Epoch 10/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 43.1890 - val_loss: 42.8638\n",
            "Epoch 11/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 42.8645 - val_loss: 43.2334\n",
            "Epoch 12/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 42.6437 - val_loss: 42.6243\n",
            "Epoch 13/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 42.5053 - val_loss: 42.0224\n",
            "Epoch 14/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 42.2490 - val_loss: 42.8112\n",
            "Epoch 15/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 42.0914 - val_loss: 42.1545\n",
            "Epoch 16/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 41.9104 - val_loss: 42.2021\n",
            "Epoch 17/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 41.7602 - val_loss: 41.9493\n",
            "Epoch 18/200\n",
            "1875/1875 [==============================] - 31s 17ms/step - loss: 41.6487 - val_loss: 41.6115\n",
            "Epoch 19/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 41.4459 - val_loss: 41.6745\n",
            "Epoch 20/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 41.3591 - val_loss: 41.7346\n",
            "Epoch 21/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 41.2529 - val_loss: 41.7806\n",
            "Epoch 22/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 41.1546 - val_loss: 41.2874\n",
            "Epoch 23/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 41.0445 - val_loss: 41.1144\n",
            "Epoch 24/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 40.9514 - val_loss: 41.0617\n",
            "Epoch 25/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 40.8994 - val_loss: 41.5448\n",
            "Epoch 26/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 40.7702 - val_loss: 40.7816\n",
            "Epoch 27/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 40.7085 - val_loss: 41.2402\n",
            "Epoch 28/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 40.6087 - val_loss: 40.8660\n",
            "Epoch 29/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 40.5566 - val_loss: 41.0116\n",
            "Epoch 30/200\n",
            "1875/1875 [==============================] - 30s 16ms/step - loss: 40.4583 - val_loss: 41.1775\n",
            "Epoch 31/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 40.4184 - val_loss: 40.7022\n",
            "Epoch 32/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 40.3504 - val_loss: 41.1581\n",
            "Epoch 33/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 40.2942 - val_loss: 40.4489\n",
            "Epoch 34/200\n",
            "1875/1875 [==============================] - 31s 17ms/step - loss: 40.2187 - val_loss: 40.6264\n",
            "Epoch 35/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 40.1798 - val_loss: 40.7075\n",
            "Epoch 36/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 40.1210 - val_loss: 40.6276\n",
            "Epoch 37/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 40.0732 - val_loss: 40.3527\n",
            "Epoch 38/200\n",
            "1875/1875 [==============================] - 31s 17ms/step - loss: 39.9841 - val_loss: 40.9924\n",
            "Epoch 39/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 39.9654 - val_loss: 40.6769\n",
            "Epoch 40/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 39.8929 - val_loss: 40.6088\n",
            "Epoch 41/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 39.8633 - val_loss: 40.2340\n",
            "Epoch 42/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 39.8218 - val_loss: 40.4890\n",
            "Epoch 43/200\n",
            "1875/1875 [==============================] - 31s 17ms/step - loss: 39.7686 - val_loss: 40.4952\n",
            "Epoch 44/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 39.7094 - val_loss: 40.5194\n",
            "Epoch 45/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 39.6914 - val_loss: 40.3354\n",
            "Epoch 46/200\n",
            "1875/1875 [==============================] - 31s 17ms/step - loss: 39.6277 - val_loss: 40.0706\n",
            "Epoch 47/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 39.6249 - val_loss: 40.1658\n",
            "Epoch 48/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 39.5603 - val_loss: 40.1346\n",
            "Epoch 49/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 39.5087 - val_loss: 39.8738\n",
            "Epoch 50/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 39.5071 - val_loss: 40.5336\n",
            "Epoch 51/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 39.4626 - val_loss: 40.2179\n",
            "Epoch 52/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 39.4202 - val_loss: 40.1311\n",
            "Epoch 53/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 39.3930 - val_loss: 39.8886\n",
            "Epoch 54/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 39.3446 - val_loss: 40.2365\n",
            "Epoch 55/200\n",
            "1875/1875 [==============================] - 31s 17ms/step - loss: 39.3261 - val_loss: 40.0264\n",
            "Epoch 56/200\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 39.2875 - val_loss: 40.0112\n",
            "Epoch 57/200\n",
            " 998/1875 [==============>...............] - ETA: 13s - loss: 39.2633"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving the model"
      ],
      "metadata": {
        "id": "KROWYNi0EkgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.save(\"VAE_encoder.h5\")\n",
        "decoder.save(\"VAE_decoder.h5\")\n",
        "vae.save(\"VAE.h5\")"
      ],
      "metadata": {
        "id": "2vDxZk3y4DZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test the Model"
      ],
      "metadata": {
        "id": "N2HYY2qvEnZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = tensorflow.keras.models.load_model(\"VAE_encoder.h5\", compile=False)\n",
        "decoder = tensorflow.keras.models.load_model(\"VAE_decoder.h5\", compile=False)\n",
        "\n",
        "# Preparing MNIST Dataset\n",
        "(x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.mnist.load_data()\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "x_test = numpy.reshape(x_test, newshape=(x_test.shape[0], x_train.shape[1], x_train.shape[2], 1))\n",
        "\n",
        "encoded_data = encoder.predict(x_test)\n",
        "decoded_data = decoder.predict(encoded_data)"
      ],
      "metadata": {
        "id": "-YPvJ-7wEj_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LN1G4fF1opqr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}